# ğŸ§ª A/B Prompt Testing with Langfuse + Ministral 14B

> Compare two prompt strategies side-by-side using Langfuse's experiment framework and Ministral 14B.

---

## ğŸ“ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LANGFUSE                           â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  Prompt v1   â”‚    â”‚  Prompt v2   â”‚                   â”‚
â”‚  â”‚  label: "a"  â”‚    â”‚  label: "b"  â”‚                   â”‚
â”‚  â”‚  Concise     â”‚    â”‚  Detailed    â”‚                   â”‚
â”‚  â”‚  temp: 0.3   â”‚    â”‚  temp: 0.7   â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚         â”‚                   â”‚                           â”‚
â”‚         â–¼                   â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚     Dataset: ab-test-eval-set       â”‚                â”‚
â”‚  â”‚     8 questions + expected output   â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                 â”‚                                       â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚      â–¼                     â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚  Run: A    â”‚    â”‚  Run: B    â”‚                       â”‚
â”‚  â”‚  8 traces  â”‚    â”‚  8 traces  â”‚                       â”‚
â”‚  â”‚  + scores  â”‚    â”‚  + scores  â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                         â”‚
â”‚            ğŸ“Š Compare in Dashboard                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Ministral 14B    â”‚
              â”‚  ministral-14b-   â”‚
              â”‚  latest           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ†š The Two Prompts

| | **Prompt A** | **Prompt B** |
|---|---|---|
| **Label** | `a` | `b` |
| **Strategy** | Concise & direct | Chain-of-thought |
| **System prompt** | _"Answer concisely in at most 2 sentences."_ | _"Think step by step. Provide a detailed explanation with examples."_ |
| **Temperature** | `0.3` | `0.7` |
| **Expected behavior** | Short, to-the-point answers | Longer, reasoned explanations |

---

## ğŸ“‹ Evaluation Dataset

The dataset `ab-test-eval-set` contains 8 programming questions in Portuguese:

| # | Question | Expected Answer (summary) |
|---|---|---|
| 1 | What is recursion in programming? | Function that calls itself |
| 2 | Explain what a REST API is | Interface using HTTP + REST principles |
| 3 | Difference between list and tuple in Python? | Lists are mutable, tuples are not |
| 4 | What is Big O notation? | Describes algorithm complexity |
| 5 | Concept of closure in JavaScript? | Function accessing outer scope after return |
| 6 | What is a relational database? | Data in tables with relations |
| 7 | What is Docker used for? | Containers packaging apps + dependencies |
| 8 | What is machine learning? | Algorithms learning patterns from data |

---

## ğŸ“ Scoring Metrics

Each response is evaluated with two metrics:

| Metric | What it measures | Score range | Ideal |
|---|---|---|---|
| **keyword_overlap** | % of expected keywords found in the response | `0.0` â€“ `1.0` | `1.0` |
| **response_length** | Penalizes too short (<50 chars) or too long (>500 chars) responses | `0.0` â€“ `1.0` | `1.0` (50â€“500 chars) |

Scores are recorded in Langfuse per trace, enabling side-by-side comparison in the dashboard.

---

## ğŸš€ Quick Start

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure environment

```bash
cp .env.example .env
```

Fill in your keys:

```env
LANGFUSE_SECRET_KEY=sk-lf-...
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_HOST=https://cloud.langfuse.com
MISTRAL_API_KEY=...
```

### 3. Create prompts + dataset in Langfuse

```bash
python setup_langfuse.py
```

### 4. Run the A/B test

```bash
python run_ab_test.py
```

---

## ğŸ“Š Sample Output

```
==================================================
Running experiment: ab-test-prompt-a
==================================================

  Q: What is recursion in programming?
  A: Recursion is when a function calls itself...

  Q: Explain what a REST API is.
  A: A REST API is an interface for communication...

  ...

============================================================
RESULTS COMPARISON
============================================================
Metric                       Prompt A     Prompt B     Winner
------------------------------------------------------------
avg_keyword_overlap             0.625        0.750          B
avg_response_length             0.950        0.820          A
```

---

## ğŸ“ Project Structure

```
a-b-prompt-testing/
â”‚
â”œâ”€â”€ .env.example        # Environment variables template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚
â”œâ”€â”€ setup_langfuse.py   # Seeds prompts (a/b) + dataset into Langfuse
â””â”€â”€ run_ab_test.py      # Runs both experiments and compares results
```

---

## ğŸ”— How Langfuse Tracks Everything

```
Prompt "a" â”€â”€â–º compile(question) â”€â”€â–º Ministral API â”€â”€â–º response â”€â”€â–º score â”€â”€â–º Langfuse Trace
                                                                              â”‚
Prompt "b" â”€â”€â–º compile(question) â”€â”€â–º Ministral API â”€â”€â–º response â”€â”€â–º score â”€â”€â–º Langfuse Trace
                                                                              â”‚
                                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                    â–¼
                                                          Langfuse Dashboard
                                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                          â”‚ Dataset Runs    â”‚
                                                          â”‚ â”œâ”€ prompt-a     â”‚
                                                          â”‚ â””â”€ prompt-b     â”‚
                                                          â”‚                 â”‚
                                                          â”‚ Compare scores  â”‚
                                                          â”‚ side by side    â”‚
                                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ›  Tech Stack

| Component | Technology |
|---|---|
| LLM | Ministral 14B (`ministral-14b-latest`) |
| Observability | Langfuse (prompt management + experiments) |
| Language | Python 3.10+ |
| Prompt management | Langfuse prompt versioning with labels |
| Evaluation | Custom keyword overlap + response length scorers |
